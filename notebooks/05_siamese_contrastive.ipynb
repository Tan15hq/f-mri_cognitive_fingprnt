{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet torch torchvision --disable-pip-version-check\n"
      ],
      "metadata": {
        "id": "4ZkdDhbE2YFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "WORKDIR = '/content/drive/MyDrive/fmri_fingerprint'\n",
        "RESULTS_DIR = os.path.join(WORKDIR, 'results')\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "D1KnJl622Zrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs = 120\n",
        "lr = 1e-3\n",
        "embedding_dim = 64\n",
        "temperature = 0.1\n",
        "topK = 500\n",
        "seed = 42\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n"
      ],
      "metadata": {
        "id": "mZgMUZEU2b8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data prep (construct v1, v2 if needed)\n",
        "try:\n",
        "    v1; v2\n",
        "    print(\"Using existing v1/v2 variables.\")\n",
        "except Exception:\n",
        "    try:\n",
        "        Z1_arr\n",
        "        Z2_arr\n",
        "        iu\n",
        "        print(\"Constructing v1/v2 from Z1_arr/Z2_arr and iu.\")\n",
        "        v1 = Z1_arr[:, iu[0], iu[1]].astype(np.float32)\n",
        "        v2 = Z2_arr[:, iu[0], iu[1]].astype(np.float32)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Could not find v1/v2 nor Z1_arr/Z2_arr+iu. Run split-half cells first.\") from e\n",
        "\n",
        "n_subj, n_edges = v1.shape\n",
        "print(f\"n_subj={n_subj}, n_edges={n_edges}\")\n"
      ],
      "metadata": {
        "id": "445xVldh2dYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_idx = None\n",
        "if (('icc_vals' in globals() or 'icc_vals' in locals()) and topK is not None):\n",
        "    try:\n",
        "        order = np.argsort(icc_vals)[::-1]\n",
        "        selected_idx = order[:min(topK, len(order))]\n",
        "        v1_sel = v1[:, selected_idx]\n",
        "        v2_sel = v2[:, selected_idx]\n",
        "        use_idx = selected_idx\n",
        "        print(f\"Using top-{len(selected_idx)} ICC edges (selected by icc_vals).\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to use icc_vals selection; falling back to all edges.\", e)\n",
        "        v1_sel = v1\n",
        "        v2_sel = v2\n",
        "else:\n",
        "    v1_sel = v1\n",
        "    v2_sel = v2\n",
        "    print(\"Using all edges (no ICC-based selection).\")\n"
      ],
      "metadata": {
        "id": "GXCVjkq72fCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X_all_views = np.vstack([v1_sel, v2_sel])  # 2N x F\n",
        "scaler = StandardScaler()\n",
        "X_all_views = scaler.fit_transform(X_all_views)\n",
        "v1_norm = X_all_views[:n_subj]\n",
        "v2_norm = X_all_views[n_subj:]\n"
      ],
      "metadata": {
        "id": "QCVERgNw2g1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitHalfPairs(Dataset):\n",
        "    def __init__(self, v1_array, v2_array):\n",
        "        assert v1_array.shape == v2_array.shape\n",
        "        self.v1 = torch.tensor(v1_array, dtype=torch.float32)\n",
        "        self.v2 = torch.tensor(v2_array, dtype=torch.float32)\n",
        "        self.N = self.v1.shape[0]\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "    def __getitem__(self, idx):\n",
        "        return self.v1[idx], self.v2[idx], idx\n",
        "\n",
        "dataset = SplitHalfPairs(v1_norm, v2_norm)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "TbrtRFKK2iZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, emb_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = F.normalize(z, p=2, dim=1)\n",
        "        return z\n",
        "\n",
        "input_dim = v1_norm.shape[1]\n",
        "model = MLPEncoder(input_dim, embedding_dim).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "EtH4TFJh2j37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nt_xent_loss(z1, z2, temp=0.1):\n",
        "    B = z1.shape[0]\n",
        "    z = torch.cat([z1, z2], dim=0)\n",
        "    sim = torch.matmul(z, z.T) / temp\n",
        "    diag_mask = torch.eye(2*B, device=z.device).bool()\n",
        "    sim_masked = sim.masked_fill(diag_mask, -9e15)\n",
        "    log_prob = sim_masked - torch.logsumexp(sim_masked, dim=1, keepdim=True)\n",
        "    pos_logprob = torch.cat([log_prob[torch.arange(B), torch.arange(B)+B],\n",
        "                             log_prob[torch.arange(B)+B, torch.arange(B)]], dim=0)\n",
        "    loss = - pos_logprob.mean()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "9MiZqcyT2lNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 1e9\n",
        "patience = 20\n",
        "wait = 0\n",
        "start_time = time.time()\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for v1_batch, v2_batch, idxs in loader:\n",
        "        v1_batch = v1_batch.to(device)\n",
        "        v2_batch = v2_batch.to(device)\n",
        "        z1 = model(v1_batch)\n",
        "        z2 = model(v2_batch)\n",
        "        loss = nt_xent_loss(z1, z2, temp=temperature)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * v1_batch.shape[0]\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}  avg_loss={avg_loss:.4f}  time_elapsed={(time.time()-start_time):.1f}s\")\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), os.path.join(RESULTS_DIR, 'siamese_encoder_best.pt'))\n",
        "    else:\n",
        "        wait += 1\n",
        "    if wait >= patience:\n",
        "        print(\"Early stopping (patience reached).\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "r4Xel_l02mXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'siamese_encoder_best.pt')))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    v1_tensor = torch.tensor(v1_norm, dtype=torch.float32).to(device)\n",
        "    v2_tensor = torch.tensor(v2_norm, dtype=torch.float32).to(device)\n",
        "    emb1 = model(v1_tensor).cpu().numpy()\n",
        "    emb2 = model(v2_tensor).cpu().numpy()\n",
        "\n",
        "sim_mat = cosine_similarity(emb1, emb2)\n",
        "preds = sim_mat.argmax(axis=1)\n",
        "accuracy = (preds == np.arange(n_subj)).mean()\n",
        "print(f\"Siamese NN identification accuracy (embedding NN): {accuracy:.3f} ({int(accuracy*100)}%)\")\n",
        "\n",
        "np.save(os.path.join(RESULTS_DIR, 'siamese_emb_view1.npy'), emb1)\n",
        "np.save(os.path.join(RESULTS_DIR, 'siamese_emb_view2.npy'), emb2)\n",
        "np.save(os.path.join(RESULTS_DIR, 'siamese_sim_matrix.npy'), sim_mat)\n",
        "with open(os.path.join(RESULTS_DIR, 'siamese_report.txt'), 'w') as f:\n",
        "    f.write(f\"device: {device}\\n\")\n",
        "    f.write(f\"n_subj: {n_subj}\\n\")\n",
        "    f.write(f\"n_edges_used: {v1_sel.shape[1]}\\n\")\n",
        "    f.write(f\"embedding_dim: {embedding_dim}\\n\")\n",
        "    f.write(f\"topK_used (if any): {topK if use_idx is not None else 'all'}\\n\")\n",
        "    f.write(f\"accuracy: {accuracy}\\n\")\n",
        "print(\"Saved siamese artifacts to:\", RESULTS_DIR)\n"
      ],
      "metadata": {
        "id": "DHyL6V8H2n6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    Z12 = np.vstack([emb1, emb2])\n",
        "    Zp = pca.fit_transform(Z12)\n",
        "    Zp1 = Zp[:n_subj]; Zp2 = Zp[n_subj:]\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(Zp1[:,0], Zp1[:,1], label='view1', alpha=0.9)\n",
        "    plt.scatter(Zp2[:,0], Zp2[:,1], label='view2', marker='x', alpha=0.9)\n",
        "    for i in range(n_subj):\n",
        "        plt.plot([Zp1[i,0], Zp2[i,0]], [Zp1[i,1], Zp2[i,1]], 'k-', alpha=0.3)\n",
        "    plt.title(f\"Siamese embeddings (PCA 2D); NN acc={accuracy:.3f}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR,'siamese_embeddings_pca2d.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Skipping PCA viz (matplotlib/PCA issue):\", e)\n"
      ],
      "metadata": {
        "id": "sFPEmHOs2pXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}